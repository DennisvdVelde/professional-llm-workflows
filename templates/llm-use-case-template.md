# LLM Use Case Template

This template helps define a clear, realistic, and responsible use case for working with Large Language Models (LLMs).

It is designed to prevent vague experimentation and to make explicit:
- where an LLM adds value
- where human control is required
- what risks must be managed

Use this template before introducing LLMs into recurring professional work.

---

## 1. Task description

**What recurring task do you perform?**

Describe the task as it currently exists, without reference to tools or AI.

- What is the task?
- How often does it occur?
- What type of output does it produce?

> Keep this concrete. Broad descriptions lead to weak use cases.

---

## 2. Why this task is suitable for LLM support

**Why might an LLM help with this task?**

Consider:
- cognitive load
- repetitive structure
- need for drafting, structuring, or exploring options

Avoid statements like “to save time” without explaining *where* time is currently spent.

---

## 3. Goal of LLM support

**What should the LLM help you achieve?**

Choose one primary goal:
- structuring thinking
- generating first drafts
- exploring alternatives
- reformulating existing content
- summarizing or organizing information

LLMs should support thinking — not replace decisions.

---

## 4. Where the LLM is used in the task

**At which step(s) of the task will the LLM be involved?**

Be explicit:
- early exploration
- drafting
- restructuring
- reviewing for clarity

Do not allow the LLM to span the entire task without checkpoints.

---

## 5. Human control points

**Where do you explicitly review, correct, or decide?**

List the moments where:
- output is verified
- assumptions are checked
- decisions are made
- responsibility is exercised

If a control point cannot be clearly described, the use case is not ready.

---

## 6. What the LLM must not do

**Which actions or decisions are explicitly excluded?**

Examples:
- making final decisions
- generating factual claims without verification
- interpreting legal, financial, or medical consequences
- communicating externally without review

This section defines the boundary of responsible use.

---

## 7. Risk assessment

**What could go wrong if the LLM output is incorrect?**

Consider:
- reputational impact
- operational consequences
- decision quality
- downstream use of the output

Higher risk requires stricter control.

---

## 8. Verification approach

**How will LLM output be checked before use?**

Describe:
- manual review steps
- reference checks
- comparison against known information
- escalation when uncertainty remains

Verification is not optional — it is part of the use case.

---

## 9. Decision rule

**When is LLM output acceptable to use?**

Define clear criteria, such as:
- “only after verification X”
- “only as internal draft material”
- “never without human rewrite”

If no clear rule can be defined, the use case is unsafe.

---

## Final check

Before using this use case, confirm:

- [ ] The task is clearly defined
- [ ] The LLM’s role is limited and specific
- [ ] Human control is explicit
- [ ] Risks are understood
- [ ] Verification is built in

If any box cannot be checked, revise the use case.
